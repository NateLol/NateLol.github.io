<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>ML - Category - Guodong's Blog</title><link>https://natelol.github.io/categories/ml/</link><description>ML - Category - Guodong's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 11 Oct 2020 20:40:22 +0800</lastBuildDate><atom:link href="https://natelol.github.io/categories/ml/" rel="self" type="application/rss+xml"/><item><title>Neighbour Components Anaysis</title><link>https://natelol.github.io/nca/</link><pubDate>Sun, 11 Oct 2020 20:40:22 +0800</pubDate><author>Author</author><guid>https://natelol.github.io/nca/</guid><description><![CDATA[<div class="featured-image">
                <img src="/NCA.png" referrerpolicy="no-referrer">
            </div>Neighbour Components Analysis (NCA) learns a linear transfromation for the input space such that in the transformed space, KNN performs well.
Let $Q = A^TA$, we have $$ \begin{aligned}d(x,y) &amp;= (x-y)^T Q (x-y) \\ &amp;= (x-y)^TA^TA(x-y) \\&amp;= (Ax-Ay)^T(Ax-Ay)\end{aligned} $$ NCA proposes to use a softmax value to represent the probability that for a given sample $i$, sample $j$ is selected as its neighbour. $$ p_{ij} = \frac{\exp(-||Ax_i - Ax_j||^2)}{\sum_{k \neq i} \exp(-||Ax_i - Ax_k||^2)} $$ which is the pair similarity over all posible pairs, looking for the nearnest sample in the new space.]]></description></item></channel></rss>